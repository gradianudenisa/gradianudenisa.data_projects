
# gradianudenisa.data_projects
#__Project 1: Hacker News__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Hacker%20News)<br>
In this project we are analyzing a dataset of submissions to the Hacker News site, which is a site where posts receive votes and comments, similar to reddit. Hacker News is extremely popular in technology and startup circles, and posts that make it to the top of the Hacker News listings can get hundreds of thousands of visitors as a result. In this context we are interested to know the following:


* Do Ask HN (meaning users submiting a post to ask the Hacker News community a question) or Show HN (meaning users submiting a post to show the Hacker News community a project, product etc.) receive more comments on average?
* Do posts created at a certain time receive more comments on average

#__Project 2: Exploring Car Sales Data__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Exploring%20Car%20Sales)<br>
The goal of this project is to use and apply various data cleaning techniques to:
* transform numeric data stored as text which should be cleaned and converted, 
* removing outliers, 
* dealing with incorrect data, 
* mapping german words to their english counterparts etc.<br><br>
and to analyze and explore the included used car listings, namely:<br><br>
* exploring variations of price across different car brands, 
* exploring average mileage for top 6 cars (by price) and see if there's any visible link with mean price, 
* finding the most common brand/model combinations, 
* finding how much cheaper are cars with damage than their non-damaged counterparts etc.

#__Project 3: Heavy Traffic Indicators__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Heavy%20Traffic%20Indicators)<br>
Our analysis goal is to determine indicators of heavy traffic on I-94. After processing the data, we are going to use various exploratory data visualization techniques (like visualizing time series data with line plots, visualizing correlations with scatter plots, visualizing frequency distributions with bar plots and histograms, comparing graphs using grid charts) to investigate data and find patterns.<br><br>
After a rigorous examination, we managed to find two types of indicators of heavy traffic on I-94:

__Time indicators__
* The traffic is usually heavier during warm months compared to cold months.
* The traffic is usually heavier on business days compared to the weekends.
* On business days, the rush hours are around 7 and 16.
__Weather indicators__
* Shower snow
* Light rain and snow
* Proximity thunderstorm with drizzle

#__Project 4: Wine Quality Indicators__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Wine%20Quality%20(Matplotlib%20styles))<br>
Our goal in this project is to use Matplotlib's pre-defined style, namely the fivethirtyeight style to build a graph that shows which attributes have the strongest correlation with the wine quality. We will maximize the data-ink ratio and add various structural elements on top of the fivethirtyeight style.

#__Project 5: Analyzing Exchange Rates__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Exchange%20Rates)<br>
The objective of this project is to highlight and use various explanatory data visualization techniques:
* using information design principles (familiarity and maximizing the data-ink ratio) to create better graphs for the targeted audience;
* creating storytelling data visualizations using Matplotlib;
* guiding the audience's attention with pre-attentive attributes;
* creating visual patterns using Gestalt principles.

We will create a storytelling data visualization showing comparatively how the euro-dollar rate changed under the last three US presidents (George W. Bush (2001-2009), Barack Obama (2009-2017), and Donald Trump (2017-2021))

#__Project 6: Analyzing Exit Surveys__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Exit%20Surveys)<br>
Our goal in this project is to use various data cleaning techniques to make the data ready to be analyzed, so that we can extract valuable insights from it. 
After cleaning and transforming the data to be ready for analysis we want to answer the following:
* Are employees who only worked for the institutes for a short period of time resigning due to some kind of dissatisfaction? What about employees who have been there longer?
* Are younger employees resigning due to some kind of dissatisfaction? What about older employees?

#__Project 7: StarWars Surveys__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20StarWars%20Survey)<br>
The purpose of our analysis is to find out if the original movies are ranked higher overall and enjoy more popularity among the fans of the Star Wars series compared to the more recent movies.<br><br> The dataset is based on a survey that implied Star Wars fans (for some questions, the respondent had to check one or more boxes). This type of data is difficult to represent in a column, so before analyzing it the dataset needs some cleaning (mapping).

#__Project 8: Analyzing Factbook data SQL__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Factbook%20Data%20SQL)<br>
In this project, we'll work with data from the CIA World Factbook (the SQLite factbook.db database), a compendium of statistics about all of the countries on Earth. The object of this project is to explore the database using SQL and the Jupyter Notebook interface, filtering the data accordingly to the purpose of our analysis and finding densely populated countries.

#__Project 9: Answering Business Questions using SQL__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Business%20Questions%20SQL)<br>
In this project we will put our SQL skills to work to answer some business questions like:
* which genre sold the most tracks in USA;
* which sales agents sold the most tracks;
* what percentage of purchases are individual tracks vs whole albums.

#__Project 10: Analyzing Stack Exchange__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Analyzing%20Stack%20Exchange)<br>
The aim of the following analysis is to use Data Science Stack Exchange to determine what content should a data science education company create, based on the interest by subject. In order to find what is best content to write about we will use two popularity proxies: for each tag we'll count how many times the tag was used, and how many times a question with that tag was viewed. <br><br>
The analysis contains visualisations for the most used and most viewed tags as well as additional investigation for deep-learning where we tried to establish if the interest was just momentary or if based on a longer term analysis we can conclude the domain has the potential to remain of interest in the future.

#__Project 11: Analyzing Fandango Movie Ratings__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Fandango%20Movie%20Ratings)<br>
The context of this project is based on the work of a data journalist, Walt Hickey, who in October 2015 analyzed movie ratings data and found evidence to suggest that Fandango's rating system was biased and dishonest (discrepancy between the number of stars displayed to users and the actual rating which was almost always rounded up). In this project, we'll analyze more recent movie ratings data to determine whether there has been any change in Fandango's rating system after the analysis that we just mentioned above. <br><br>
The analysis will contain a high-level comparison between the shapes of the distributions using kernel density plots and a more granular analysis using some summary statistics to determine the direction of the change.

#__Project 12: Best Markets for E-learning__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Best%20Markets%20for%20E-learning)<br>
The goal of this project is to find the best two markets to advertise the product of an e-learning company that offers courses on programming. To be more precise, we want to answer questions about a population of new coders that are interested in the subjects they teach (mostly web and mobile development). At the end of the analysis, we would like to know:

* Where are these new coders located.
* What locations have the greatest densities of new coders.
* How much money they're willing to spend on learning.<br><br>
For this analysis we will thoroughly check for Sample Representativity and apply various statistics to summarize distributions and clean the data (dealing with outliers) to avoid incorrect results.

#__Project 13: Lottery Addiction__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Lottery%20Addiction)<br>
In this project we are going to use our knowledge in probability and combinatorics to simulate our contribution to the development of a mobile app that helps people beat lottery addiction by calculating their actual chances of winning. We will focus on the 6/49 lottery and build functions that can answer the following questions:

* What is the probability of winning the big prize with a single ticket?
* What is the probability of winning the big prize if we play 40 different tickets (or any other number)?
* What is the probability of having at least five (or four, or three) winning numbers on a single ticket?

#__Project 14: Building Spam Filter__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Building%20Spam%20Filter)<br>
In this project we're going to use the multionomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans to build a spam filter for SMS messages with an accuracy of 98.74% on the test set. The outline of our work consists in:

* teach the computer how humans classify messages
* the computer uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam
* finally the computer classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).

#__Project 14: Preparing for Jeopardy__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Preparing%20for%20Jeopardy)<br>
In this project we want to figure out which strategy has the most substantial chance of success for the TV show Jeopardy, in other words which patterns we can extract from the data that could help a contestant prepare for Jeopardy like:

* How often the answer can be used for a question;
* How often questions are repeated;
* Which terms correspond to high-value questions using a chi-squared test;
* Which domain of questions constitutes the majority for the top categories.

After analyzing the data we concluded that:
* An average of 5.8% of the words that form the answer also occur in the question
* The mean proportion of terms that occured in past questions and are recycled is 68 %
* According to the chi-squared test, we didn't find statistically significant values of frequencies of words that appear more on high value questions.
* Focusing on a particular domain of questions is not significant since they constitute only a small percentage of the total questions Therefore, from these strategies the best one would be to study past questions.

#__Project 15: Supervised ML Heart Disease__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Supervised%20ML%20Heart%20Disease
)<br>
In this project we want to build a machine learning model that will accurately predict the likelihood of a new patient having heart disease in the future using anonymized data from multiple hospitals on several patient. 

After selecting the features for our model using a Pearson's correlation heat map we experimented using hyperparameter optimization to imporve the model accuracy and we got a model accuracy on the test set of 87.68%.

#__Project 16: Unsupervised learning Credit Card Customer Segmenatation__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Supervised%20ML%20Heart%20Disease
)<br>
In this project we are going to use a dataset containing information about the clients of a company and our goal is to help segment them into different groups in order to apply different business strategies for each type of customer. We will use the K-means algorithm to segment the data. In the end we are expected to deliver a group for each client and an explanation of the characteristics of each group and the main points that make them different.

In order to use the algorithm properly and achieve all the goals presented above we'll go through the following steps:

* Analyze the dataset;
* Prepare the data for modeling;
* Find an appropriate number of clusters;
* Segment the data;
* Interpret and explain the results.

#__Project 17: Predicting Insurance Costs (Linear Regression)__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Predicting%20Insurance%20Costs%20(Linear%20Regression))<br>
The scope of this project is to build the best possible predictive model for the medical insurance cost, given some demographic or personal characteristics of patients. The cost is a continuous, positive number, which makes it a good candidate for a linear regression. 

After exploring the data, based on the statistical measure shown thourgh correlation we chose age, bmi and smoker are predictors for our model. The mean squared wrror was 1.57 on the training set and the coefficient of determination was 0.74. After we checked if the assumptions of linear regressions were met
* mean of residuals is 0
* the variance of the erros is 0

After analyzing the residuals, they suggested some violations to the assumptions of linear regression because As fitted values got larger, the residuals trend downward. 

After interpreting the model we concluded that:
* A year increase for a patient is associated with a 0.04 increase in the log charges, holding smoking status and bmi constant.
* A unit increase in the subject BMI is associated with a 0.01 increase in the log charges, holding smoking status and age constant.
* A smoker is associated with a 2.23 increase in the log charges, holding age and bmi constant. About a 930% increase in the charges on the regular scale.

 #__Project 18: Classifying Heart Disease (Logistic Regression)__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Classifying%20Heart%20Disease%20using%20Logistic%20Regression)<br>
The scope of this project is to use the machine learning pipeline, starting from examining the dataset itself to creating a polished classification model using binary logistic regression that will try to classify the presence of heart disease in an individual, given recorded information on various patient characteristics, including age, chest pain and others.

After exploring the data and making some adjustments, we divided the data, choose some predictors based on the findings found from correlations, the Split-Apply-Combine workflow, the RFE algorthm. The model was fit to the training set, and  the predictive ability was evaluated on the test set. We assessed the sensitivity, specificity, overall training and test accuracy of the model. We interpreted the model coefficients on both the log-odds and odds scales. to see if they make sense. The final model evaluation suggested an accuracy of 84%, sensitivity was 76%, and specificity was 90%.

 #__Project 19: Predicting Employee Productivity (Tree Models)__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Employee%20Productivity%20(Tree%20Models)))<br>
In this project we:
* Cleaned the data and created a target column for a classification tree which illustrates which aspects of a factory are best at predicting if a team will be productive or not.

* Built, visualized and evaluated a decision tree using cross-validation

* Explained the insights to a non-technical audience.

* Used a random forest to either improve or double check the scores of your tree.  

We discovered that the "incentive" and the "smv" variables were the ones with the greatest influence on the final prediction.We got satisfacatory results. The accuracy on the test set was 85%. We used cross-validation to evaluate the tree and we got an accuracy of 82%, a mean Cross Validated Precision of85%, a mean Cross Validated recall of 92% and a mean Cross Validated F1 score of 88%. The accuracy when using the RandomForestClassifier was 85%.

#__Project 20: Optimizing Model Prediction Forest Fires__ <br>
[Click here to redirect to code](https://github.com/gradianudenisa/gradianudenisa.data_projects/tree/main/Project%20Optimizing%20Model%20Prediction%20Forest%20Fires)<br><br>
In this project we are going to use a standard linear regression model as a reference and the goal is to iterate on it using various tools for optimizing the machine learning workflow such as handling outliers with imputation using k-Nearest Neighbors, performing k-fold cross-validation, and using regularization — among others.<br><br> After processing the data, I developed some candidate models using forward feature selection, backward feature selection, ridge model, LASSO model and spline model.In the end I evaluated the models using k-fold cross-validation since it gives us a better idea about model performance on unseen datasets compared to a single train-test split.The machine learning task was to predict the extent of fire damage to a forest.  

This repository is a collection of my guided projects from Dataquest.io. 
